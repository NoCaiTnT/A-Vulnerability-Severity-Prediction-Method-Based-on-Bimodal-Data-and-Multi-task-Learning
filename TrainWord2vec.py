# -*- coding: utf-8 -*-


import time
import pickle
import os
import pandas as pd

from gensim.models import Word2Vec
from keras.preprocessing.text import Tokenizer
from LoadCFilesAsText import getCFilesFromText

script_start_time = time.time()

print ("Script starts at: " + str(script_start_time))

working_dir = "D:\\Your\\Directory\\"

file_path = working_dir + "Path\\to\\Your\\Source\\Code\\"


def getData(filePath):
    df = pd.read_csv(filePath, sep=",", low_memory=False)
    df_list = df.values.tolist()
    temp = []
    ####
    for i in df_list:

        i = [x for x in i if str(x) != 'nan']
        temp.append(i)
    
    return temp


def ProcessList(list_to_process):
    token_list = []
    for sub_list_to_process in list_to_process:
        sub_token_list = []
        for each_word in sub_list_to_process:
            sub_word = each_word.split()
            for element in sub_word:
                sub_token_list.append(element)
        token_list.append(sub_token_list)
    return token_list


#train_token_list = getData(file_path)
train_token_list, train_token_list_id = getCFilesFromText(file_path)

train_token_list = ProcessList(train_token_list)

print ("The length of training set is : " + str(len(train_token_list)))


new_total_token_list = []

for sub_list_token in train_token_list:
    new_line = ','.join(sub_list_token)
    new_total_token_list.append(new_line)

tokenizer = Tokenizer(num_words=None, filters=',', lower=False, char_level=False)
tokenizer.fit_on_texts(new_total_token_list)


with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)



#w2vModel = Word2Vec(train_token_list, workers = 12, size=100) # With default settings, the embedding dimension is 100 and using, (sg=0), CBOW is used.  
w2vModel = Word2Vec(train_token_list, workers = 12, size=100)

print ("----------------------------------------")
print ("The trained word2vec model: ")
print (w2vModel)

w2vModel.wv.save_word2vec_format(working_dir + "w2v_model.txt", binary=False)
