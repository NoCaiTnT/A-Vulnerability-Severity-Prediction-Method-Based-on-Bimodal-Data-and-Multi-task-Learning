
import json
from collections import Counter
import gensim
import pandas as pd
import numpy as np
import parameter_config


class Dataset(object):
    def __init__(self, config):
        self.config = config
        self._dataSource = config.dataSource
        self._stopWordSource = config.stopWordSource
        self._sequenceLength = config.sequenceLength
        self._embeddingSize = config.model.embeddingSize
        self._batchSize = config.batchSize
        self._rate = config.rate
        self._stopWordDict = {}
        self.trainReviews = []
        self.trainLabels = []
        self.evalReviews = []
        self.evalLabels = []
        self.wordEmbedding = None
        self.labelList = []
    def _readData(self, filePath):

        df = pd.read_csv(filePath)  
        if self.config.numClasses == 1:
            labels = df["sentiment"].tolist() 
        elif self.config.numClasses > 1:
            labels = df["rate"].tolist()  
        review = df["review"].tolist()
        reviews = [line.strip().split() for line in review] 
        return reviews, labels
    def _labelToIndex(self, labels, label2idx):

        labelIds = [label2idx[label] for label in labels]  
        return labelIds
    def _wordToIndex(self, reviews, word2idx):

        reviewIds = [[word2idx.get(item, word2idx["UNK"]) for item in review] for review in reviews]
        # print(max(max(reviewIds)))
        # print(reviewIds)
        return reviewIds 
    def _genTrainEvalData(self, x, y, word2idx, rate):

        reviews = []
        # print(self._sequenceLength)
        # print(len(x))
        for review in x:  
            if len(review) >= self._sequenceLength:
                reviews.append(review[:self._sequenceLength])
            else:
                reviews.append(review + [word2idx["PAD"]] * (self._sequenceLength - len(review)))
                # print(len(review + [word2idx["PAD"]] * (self._sequenceLength - len(review))))

        trainIndex = int(len(x) * rate)
        trainReviews = np.asarray(reviews[:trainIndex], dtype="int64")
        trainLabels = np.array(y[:trainIndex], dtype="float32")
        evalReviews = np.asarray(reviews[trainIndex:], dtype="int64")
        evalLabels = np.array(y[trainIndex:], dtype="float32")
        return trainReviews, trainLabels, evalReviews, evalLabels

    def _getWordEmbedding(self, words):

        wordVec = gensim.models.KeyedVectors.load_word2vec_format("../word2vec/word2Vec.bin", binary=True)
        vocab = []
        wordEmbedding = []

        vocab.append("PAD")
        vocab.append("UNK")
        wordEmbedding.append(np.zeros(self._embeddingSize)) 
        wordEmbedding.append(np.random.randn(self._embeddingSize))
        # print(wordEmbedding)
        for word in words:
            try:
                vector = wordVec.wv[word]
                vocab.append(word)
                wordEmbedding.append(vector)
            except:
                print(word + "Null")
        # print(vocab[:3],wordEmbedding[:3])
        return vocab, np.array(wordEmbedding)
    def _genVocabulary(self, reviews, labels):

        allWords = [word for review in reviews for word in review]   
        subWords = [word for word in allWords if word not in self.stopWordDict]   
        wordCount = Counter(subWords)  
        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True) 
        # print(len(sortWordCount))  #161330
        # print(sortWordCount[:4],sortWordCount[-4:]) # [('movie', 41104), ('film', 36981), ('one', 24966), ('like', 19490)] [('daeseleires', 1), ('nice310', 1), ('shortsightedness', 1), ('unfairness', 1)]
        words = [item[0] for item in sortWordCount if item[1] >= 5]  
        vocab, wordEmbedding = self._getWordEmbedding(words)
        self.wordEmbedding = wordEmbedding
        word2idx = dict(zip(vocab, list(range(len(vocab))))) 
        uniqueLabel = list(set(labels))  
        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel))))) 
        self.labelList = list(range(len(uniqueLabel)))

        with open("../data/wordJson/word2idx.json", "w", encoding="utf-8") as f:
            json.dump(word2idx, f)
        with open("../data/wordJson/label2idx.json", "w", encoding="utf-8") as f:
            json.dump(label2idx, f)
        return word2idx, label2idx

    def _readStopWord(self, stopWordPath):

        with open(stopWordPath, "r") as f:
            stopWords = f.read()
            stopWordList = stopWords.splitlines()

            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))

    def dataGen(self):

        self._readStopWord(self._stopWordSource)

        reviews, labels = self._readData(self._dataSource)

        word2idx, label2idx = self._genVocabulary(reviews, labels)

        labelIds = self._labelToIndex(labels, label2idx)
        reviewIds = self._wordToIndex(reviews, word2idx)

        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx,
                                                                                    self._rate)
        self.trainReviews = trainReviews
        self.trainLabels = trainLabels

        self.evalReviews = evalReviews
        self.evalLabels = evalLabels


# config =parameter_config.Config()
# data = Dataset(config)
# data.dataGen()